# AI Quality Validation Implementation Guide

**Automated Content Quality Validator for FactsMind Carousels**

This guide implements an AI-powered quality validation layer that scores carousel content before it reaches Telegram approval. The validator uses Claude API to evaluate fact accuracy, brand voice alignment, engagement potential, length limits, and emoji usage.

---

## Table of Contents

1. [Overview](#overview)
2. [Python Script Implementation](#python-script-implementation)
3. [n8n Workflow Integration](#n8n-workflow-integration)
4. [Quality Criteria & Scoring](#quality-criteria--scoring)
5. [Auto-Rejection Logic](#auto-rejection-logic)
6. [Testing & Examples](#testing--examples)
7. [Deployment](#deployment)

---

## Overview

### What It Does

The Quality Validator analyzes carousel content against 5 key metrics:
- **Fact Accuracy** (1-10): Verifies claims are scientifically sound
- **Brand Voice** (1-10): Ensures alignment with FactsMind tone (educational, engaging, authoritative)
- **Engagement Potential** (1-10): Predicts hook strength and share-worthiness
- **Length Compliance** (Pass/Fail): Validates character limits per slide
- **Emoji Usage** (Pass/Fail): Checks appropriate emoji density

### Workflow Position

```
Groq (Generate Fact)
    ‚Üì
Gemini (Content + Images)
    ‚Üì
Python Composite (Render Slides)
    ‚Üì
üÜï QUALITY VALIDATOR (This implementation)
    ‚îú‚îÄ PASS ‚Üí Telegram (Manual Approval)
    ‚îî‚îÄ FAIL ‚Üí Error Notification + Log
```

### Tech Stack

- **Python 3.12** (already installed in n8n container)
- **Anthropic Claude API** (claude-3-5-sonnet-20241022)
- **n8n Execute Command Node** (integrated into existing workflow)
- **JSON output** for easy parsing and decision logic

---

## Python Script Implementation

### File Location

Create: `/srv/projects/faceless_prod/scripts/quality_validator.py`

This will be accessible inside the n8n container at `/data/scripts/quality_validator.py`

### Complete Script

```python
#!/usr/bin/env python3
"""
FactsMind Content Quality Validator

Validates carousel content using Claude API before posting to Instagram.
Scores content on fact accuracy, brand voice, engagement, length, and emoji usage.

Usage:
    python3 quality_validator.py <carousel_json_path> [--threshold <score>] [--output <path>]

Arguments:
    carousel_json_path: Path to JSON file containing carousel data
    --threshold: Minimum total score to pass (default: 35/50)
    --output: Path to save detailed validation report (default: stdout)

Environment Variables:
    ANTHROPIC_API_KEY: Required for Claude API access
"""

import os
import sys
import json
import argparse
from datetime import datetime
from typing import Dict, List, Any, Tuple

try:
    from anthropic import Anthropic
except ImportError:
    print("ERROR: anthropic package not installed. Run: pip3 install anthropic", file=sys.stderr)
    sys.exit(1)


class QualityValidator:
    """Validates carousel content quality using Claude API"""

    # Quality criteria weights
    WEIGHTS = {
        'fact_accuracy': 3.0,      # Most important - must be factually correct
        'brand_voice': 2.0,        # Strong brand consistency
        'engagement': 2.0,         # High engagement potential
        'length_compliance': 1.5,  # Proper formatting
        'emoji_usage': 1.5         # Appropriate emoji density
    }

    # Length limits (characters per slide type)
    LENGTH_LIMITS = {
        'hook': {'title': 80, 'subtitle': 120},
        'reveal': {'title': 80, 'subtitle': 140},
        'cta': {'title': 60, 'subtitle': 100}
    }

    # Emoji density limits (per 100 characters)
    EMOJI_LIMITS = {
        'min_density': 0.5,   # At least 1 emoji per 200 chars
        'max_density': 4.0    # No more than 4 emojis per 100 chars
    }

    def __init__(self, api_key: str = None):
        """Initialize validator with Claude API client"""
        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment")

        self.client = Anthropic(api_key=self.api_key)
        self.model = "claude-3-5-sonnet-20241022"

    def validate_carousel(self, carousel_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate complete carousel content

        Args:
            carousel_data: Dictionary containing carousel slides and metadata

        Returns:
            Validation report with scores and recommendations
        """
        print("üîç Starting quality validation...", file=sys.stderr)

        # Extract carousel components
        slides = carousel_data.get('carousel', [])
        fact = carousel_data.get('fact', {})
        hashtags = carousel_data.get('hashtags', {})

        # Initialize report
        report = {
            'timestamp': datetime.now().isoformat(),
            'carousel_id': carousel_data.get('id', 'unknown'),
            'scores': {},
            'details': {},
            'passed': False,
            'total_score': 0,
            'max_score': 50,
            'threshold': 35
        }

        # Run all validation checks
        report['scores']['fact_accuracy'] = self._validate_fact_accuracy(fact)
        report['scores']['brand_voice'] = self._validate_brand_voice(slides)
        report['scores']['engagement'] = self._validate_engagement(slides, fact)
        report['scores']['length_compliance'] = self._validate_length_compliance(slides)
        report['scores']['emoji_usage'] = self._validate_emoji_usage(slides)

        # Calculate weighted total
        total = sum(
            score * self.WEIGHTS[metric]
            for metric, score in report['scores'].items()
        )
        report['total_score'] = round(total, 2)

        # Pass/fail determination
        report['passed'] = report['total_score'] >= report['threshold']

        print(f"‚úì Validation complete: {report['total_score']}/{report['max_score']}", file=sys.stderr)

        return report

    def _validate_fact_accuracy(self, fact: Dict[str, Any]) -> float:
        """
        Validate fact accuracy using Claude

        Checks:
        - Scientific accuracy
        - Source credibility
        - Claim verifiability
        - Misleading framing

        Returns: Score 1-10
        """
        print("  ‚ö° Checking fact accuracy...", file=sys.stderr)

        prompt = f"""You are a fact-checking expert. Evaluate the accuracy of this claim:

FACT: {fact.get('fact', 'N/A')}
CATEGORY: {fact.get('category', 'N/A')}
SOURCE: {fact.get('source_url', 'Not provided')}

Rate this fact on accuracy (1-10 scale):
- 10: Completely accurate, well-established science
- 7-9: Mostly accurate with minor simplifications
- 4-6: Partially accurate but missing nuance
- 1-3: Misleading or unverified

Consider:
1. Is the claim scientifically supported?
2. Is the source credible?
3. Are there misleading simplifications?
4. Would experts in this field agree?

Respond in JSON format:
{{
    "score": <1-10>,
    "reasoning": "<2-3 sentence explanation>",
    "concerns": ["<any accuracy issues>"]
}}"""

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=500,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )

            result = json.loads(response.content[0].text)
            score = float(result.get('score', 0))

            self._store_detail('fact_accuracy', result)
            return min(10.0, max(1.0, score))

        except Exception as e:
            print(f"  ‚ö†Ô∏è  Fact accuracy check failed: {e}", file=sys.stderr)
            return 5.0  # Default to neutral score on error

    def _validate_brand_voice(self, slides: List[Dict[str, Any]]) -> float:
        """
        Validate brand voice alignment using Claude

        FactsMind brand voice:
        - Educational but accessible
        - Authoritative but not condescending
        - Engaging without being clickbait
        - Science-focused, professional tone
        - Avoids: emojis in titles, excessive hype, vague claims

        Returns: Score 1-10
        """
        print("  ‚ö° Checking brand voice alignment...", file=sys.stderr)

        slide_texts = "\n\n".join([
            f"Slide {s.get('slide_number', i+1)}: {s.get('title', '')} | {s.get('subtitle', '')}"
            for i, s in enumerate(slides)
        ])

        prompt = f"""You are a brand voice expert for FactsMind, an educational Instagram account.

BRAND VOICE GUIDELINES:
- Educational but accessible (explain complex ideas simply)
- Authoritative but not condescending (expert without being preachy)
- Engaging without clickbait (genuine curiosity, not hype)
- Professional sci-fi documentary tone
- Avoid: excessive emojis, hype words, vague claims, sensationalism

CAROUSEL CONTENT:
{slide_texts}

Rate brand voice alignment (1-10 scale):
- 10: Perfect FactsMind voice
- 7-9: Strong alignment with minor tweaks needed
- 4-6: Partially aligned but needs refinement
- 1-3: Off-brand or inappropriate tone

Respond in JSON format:
{{
    "score": <1-10>,
    "reasoning": "<2-3 sentence explanation>",
    "improvements": ["<specific suggestions if score < 8>"]
}}"""

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=500,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )

            result = json.loads(response.content[0].text)
            score = float(result.get('score', 0))

            self._store_detail('brand_voice', result)
            return min(10.0, max(1.0, score))

        except Exception as e:
            print(f"  ‚ö†Ô∏è  Brand voice check failed: {e}", file=sys.stderr)
            return 5.0

    def _validate_engagement(self, slides: List[Dict[str, Any]], fact: Dict[str, Any]) -> float:
        """
        Validate engagement potential using Claude

        Checks:
        - Hook strength (slide 1)
        - Curiosity gap
        - Shareability
        - CTA effectiveness (slide 5)

        Returns: Score 1-10
        """
        print("  ‚ö° Checking engagement potential...", file=sys.stderr)

        hook = slides[0] if slides else {}
        cta = slides[-1] if slides else {}

        prompt = f"""You are a social media engagement expert specializing in educational Instagram content.

HOOK (Slide 1):
Title: {hook.get('title', '')}
Subtitle: {hook.get('subtitle', '')}

CTA (Slide 5):
Title: {cta.get('title', '')}
Subtitle: {cta.get('subtitle', '')}

FACT: {fact.get('fact', '')}

Rate engagement potential (1-10 scale):
- 10: Highly shareable, strong curiosity gap
- 7-9: Good hook and CTA, likely to engage
- 4-6: Average engagement potential
- 1-3: Weak hook or unclear value

Consider:
1. Does the hook create curiosity?
2. Is the fact surprising or counterintuitive?
3. Would viewers share this with friends?
4. Is the CTA clear and compelling?

Respond in JSON format:
{{
    "score": <1-10>,
    "reasoning": "<2-3 sentence explanation>",
    "hook_strength": <1-10>,
    "shareability": <1-10>
}}"""

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=500,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )

            result = json.loads(response.content[0].text)
            score = float(result.get('score', 0))

            self._store_detail('engagement', result)
            return min(10.0, max(1.0, score))

        except Exception as e:
            print(f"  ‚ö†Ô∏è  Engagement check failed: {e}", file=sys.stderr)
            return 5.0

    def _validate_length_compliance(self, slides: List[Dict[str, Any]]) -> float:
        """
        Validate character length compliance

        Returns: 10 if all pass, 0 if any fail, proportional otherwise
        """
        print("  ‚ö° Checking length compliance...", file=sys.stderr)

        violations = []
        total_checks = 0
        passed_checks = 0

        for slide in slides:
            slide_type = slide.get('type', 'reveal')
            limits = self.LENGTH_LIMITS.get(slide_type, self.LENGTH_LIMITS['reveal'])

            title = slide.get('title', '')
            subtitle = slide.get('subtitle', '')

            # Check title length
            total_checks += 1
            if len(title) <= limits['title']:
                passed_checks += 1
            else:
                violations.append(
                    f"Slide {slide.get('slide_number')}: Title too long ({len(title)}/{limits['title']})"
                )

            # Check subtitle length
            total_checks += 1
            if len(subtitle) <= limits['subtitle']:
                passed_checks += 1
            else:
                violations.append(
                    f"Slide {slide.get('slide_number')}: Subtitle too long ({len(subtitle)}/{limits['subtitle']})"
                )

        score = (passed_checks / total_checks) * 10 if total_checks > 0 else 10

        self._store_detail('length_compliance', {
            'score': score,
            'violations': violations,
            'passed': f"{passed_checks}/{total_checks}"
        })

        return round(score, 2)

    def _validate_emoji_usage(self, slides: List[Dict[str, Any]]) -> float:
        """
        Validate emoji density and appropriateness

        Rules:
        - Emojis should enhance, not distract
        - Density: 0.5-4.0 emojis per 100 characters
        - No emojis in titles (professional appearance)

        Returns: Score 1-10
        """
        print("  ‚ö° Checking emoji usage...", file=sys.stderr)

        violations = []
        total_checks = 0
        passed_checks = 0

        def count_emojis(text: str) -> int:
            """Count emoji characters in text"""
            import re
            emoji_pattern = re.compile(
                "[\U0001F600-\U0001F64F"  # Emoticons
                "\U0001F300-\U0001F5FF"  # Symbols & pictographs
                "\U0001F680-\U0001F6FF"  # Transport & map
                "\U0001F1E0-\U0001F1FF"  # Flags
                "\U00002702-\U000027B0"  # Dingbats
                "\U000024C2-\U0001F251"  # Enclosed characters
                "]+", flags=re.UNICODE
            )
            return len(emoji_pattern.findall(text))

        for slide in slides:
            slide_num = slide.get('slide_number', '?')
            title = slide.get('title', '')
            subtitle = slide.get('subtitle', '')

            # Check no emojis in title
            total_checks += 1
            title_emojis = count_emojis(title)
            if title_emojis == 0:
                passed_checks += 1
            else:
                violations.append(f"Slide {slide_num}: {title_emojis} emoji(s) in title (should be 0)")

            # Check emoji density in subtitle
            total_checks += 1
            subtitle_emojis = count_emojis(subtitle)
            density = (subtitle_emojis / len(subtitle) * 100) if len(subtitle) > 0 else 0

            if self.EMOJI_LIMITS['min_density'] <= density <= self.EMOJI_LIMITS['max_density']:
                passed_checks += 1
            else:
                violations.append(
                    f"Slide {slide_num}: Emoji density {density:.1f}/100 chars "
                    f"(target: {self.EMOJI_LIMITS['min_density']}-{self.EMOJI_LIMITS['max_density']})"
                )

        score = (passed_checks / total_checks) * 10 if total_checks > 0 else 10

        self._store_detail('emoji_usage', {
            'score': score,
            'violations': violations,
            'passed': f"{passed_checks}/{total_checks}"
        })

        return round(score, 2)

    def _store_detail(self, category: str, data: Any):
        """Store detailed results for later reporting"""
        if not hasattr(self, '_details'):
            self._details = {}
        self._details[category] = data


def load_carousel_data(filepath: str) -> Dict[str, Any]:
    """Load carousel JSON data from file"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"ERROR: File not found: {filepath}", file=sys.stderr)
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"ERROR: Invalid JSON in {filepath}: {e}", file=sys.stderr)
        sys.exit(1)


def main():
    """Main execution function"""
    parser = argparse.ArgumentParser(
        description='Validate FactsMind carousel content quality using AI'
    )
    parser.add_argument(
        'carousel_json',
        help='Path to carousel JSON file'
    )
    parser.add_argument(
        '--threshold',
        type=float,
        default=35.0,
        help='Minimum total score to pass (default: 35/50)'
    )
    parser.add_argument(
        '--output',
        help='Path to save validation report JSON (default: stdout)'
    )

    args = parser.parse_args()

    # Load carousel data
    carousel_data = load_carousel_data(args.carousel_json)

    # Run validation
    validator = QualityValidator()
    report = validator.validate_carousel(carousel_data)
    report['threshold'] = args.threshold
    report['passed'] = report['total_score'] >= args.threshold

    # Add detailed findings
    if hasattr(validator, '_details'):
        report['details'] = validator._details

    # Output report
    report_json = json.dumps(report, indent=2, ensure_ascii=False)

    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(report_json)
        print(f"\n‚úÖ Report saved to: {args.output}", file=sys.stderr)
    else:
        print(report_json)

    # Exit code based on pass/fail
    exit_code = 0 if report['passed'] else 1

    status = "‚úÖ PASSED" if report['passed'] else "‚ùå FAILED"
    print(f"\n{status}: {report['total_score']:.2f}/{report['max_score']} (threshold: {args.threshold})", file=sys.stderr)

    sys.exit(exit_code)


if __name__ == '__main__':
    main()
```

---

## n8n Workflow Integration

### Step 1: Install Python Dependencies

The n8n container already has Python 3.12, but needs the Anthropic SDK.

**Add to `/srv/docker/n8n.Dockerfile`:**

```dockerfile
# Install Anthropic SDK for quality validation
RUN pip3 install --no-cache-dir anthropic==0.39.0
```

**Rebuild container:**

```bash
~/ssh-nexus 'cd /srv/docker && sudo docker compose build n8n && sudo docker compose up -d n8n'
```

### Step 2: Add Environment Variable

**Edit `/srv/docker/docker-compose.yml`:**

```yaml
services:
  n8n:
    environment:
      # ... existing vars ...
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}  # Add this line
```

**Add to `/srv/docker/.env`:**

```bash
ANTHROPIC_API_KEY=sk-ant-api03-your-key-here
```

### Step 3: Add Validation Node to Workflow

**Position:** After "Python Composite Script" node, before "Telegram" node

**Node Type:** Execute Command

**Node Configuration:**

```javascript
// Node: Quality Validator
// Type: Execute Command

// Command
python3 /data/scripts/quality_validator.py /tmp/carousel_data.json --output /tmp/validation_report.json

// Parameters
{
  "command": "python3 /data/scripts/quality_validator.py /tmp/carousel_data.json --threshold 35 --output /tmp/validation_report.json",
  "wait_for_completion": true
}

// Error handling
"continueOnFail": false,
"onError": "stopWorkflow"
```

### Step 4: Add Decision Node

**Node Type:** IF Node (Switch)

**Node Configuration:**

```javascript
// Node: Quality Check Decision
// Type: IF

// Condition
{{
  const fs = require('fs');
  const report = JSON.parse(fs.readFileSync('/tmp/validation_report.json', 'utf8'));
  return report.passed;
}}

// Routes
- Route 1 (TRUE): ‚Üí Telegram Node (manual approval)
- Route 2 (FALSE): ‚Üí Error Notification Node
```

### Step 5: Add Error Notification Node

**Node Type:** HTTP Request (to Telegram or webhook)

**Configuration:**

```javascript
// Node: Quality Validation Failed
// Type: HTTP Request

// Method: POST
// URL: https://api.telegram.org/bot{{$env.TELEGRAM_BOT_TOKEN}}/sendMessage

// Body
{
  "chat_id": "{{$env.TELEGRAM_CHAT_ID}}",
  "text": "‚ö†Ô∏è Quality Validation FAILED\n\nCarousel rejected by AI validator.\n\nScore: {{$json.total_score}}/50\nThreshold: 35\n\nReasons:\n{{$json.details}}"
}
```

### Updated Workflow Diagram

```
Groq (Generate Fact)
    ‚Üì
Gemini (Content + Images)
    ‚Üì
Python Composite (Render Slides)
    ‚Üì
üÜï Quality Validator Node
    ‚Üì
üÜï IF Node (Quality Check)
    ‚îú‚îÄ PASS (score ‚â• 35) ‚Üí Telegram Approval
    ‚îî‚îÄ FAIL (score < 35) ‚Üí Error Notification
```

---

## Quality Criteria & Scoring

### Scoring System

| Metric | Weight | Max Score | Description |
|--------|--------|-----------|-------------|
| Fact Accuracy | 3.0x | 30 | Scientific correctness, source credibility |
| Brand Voice | 2.0x | 20 | FactsMind tone alignment |
| Engagement | 2.0x | 20 | Hook strength, shareability |
| Length Compliance | 1.5x | 15 | Character limit adherence |
| Emoji Usage | 1.5x | 15 | Appropriate emoji density |
| **TOTAL** | - | **100** | Weighted sum |

**Pass Threshold:** 35/50 raw score (70% weighted)

### Fact Accuracy (30 points max)

**Claude evaluates:**
- Scientific consensus alignment
- Source credibility
- Verifiability of claims
- Misleading simplifications

**Scoring:**
- 10: Well-established, verified science
- 7-9: Accurate with minor simplifications
- 4-6: Partially accurate, needs nuance
- 1-3: Unverified or misleading

**Example PASS:**
```
Fact: "Octopuses have three hearts and blue blood"
Score: 10 - Well-documented marine biology fact
```

**Example FAIL:**
```
Fact: "Humans only use 10% of their brain"
Score: 2 - Debunked myth, scientifically inaccurate
```

### Brand Voice (20 points max)

**FactsMind voice guidelines:**
- Educational but accessible
- Authoritative without condescension
- Engaging without clickbait
- Professional sci-fi documentary tone

**Red flags:**
- Excessive hype ("INSANE!", "SHOCKING!")
- Vague claims ("Scientists say...")
- Condescending tone ("You probably didn't know...")
- Emojis in titles

**Example PASS:**
```
Title: "The Octopus Paradox"
Subtitle: "Three hearts pump copper-based blood through their alien biology"
Score: 9 - Professional, intriguing, factual
```

**Example FAIL:**
```
Title: "ü§Ø MIND-BLOWING Discovery!!"
Subtitle: "You won't BELIEVE what scientists found!!!"
Score: 3 - Clickbait, hype-focused, off-brand
```

### Engagement Potential (20 points max)

**Claude evaluates:**
- Hook strength (Slide 1)
- Curiosity gap creation
- Shareability factor
- CTA effectiveness (Slide 5)

**High engagement traits:**
- Counterintuitive facts
- Visual concepts
- "Did you know" factor
- Clear next steps

**Example PASS:**
```
Hook: "Why Do Octopuses Have Blue Blood?"
Engagement: 9 - Creates curiosity, unique angle
```

**Example FAIL:**
```
Hook: "Octopus Facts"
Engagement: 4 - Generic, no curiosity gap
```

### Length Compliance (15 points max)

**Character limits:**

| Slide Type | Title Max | Subtitle Max |
|------------|-----------|--------------|
| Hook (1) | 80 | 120 |
| Reveal (2-4) | 80 | 140 |
| CTA (5) | 60 | 100 |

**Scoring:** Proportional (10 = all pass, 0 = all fail)

### Emoji Usage (15 points max)

**Rules:**
- ‚ùå **No emojis in titles** (maintains professionalism)
- ‚úÖ **0.5-4.0 emojis per 100 chars in subtitles** (enhances without distracting)

**Example PASS:**
```
Title: "The Octopus Paradox"
Subtitle: "Three hearts ü´Ä pump copper-based blood ü©∏ through alien biology"
Density: 2.7/100 chars ‚úÖ
```

**Example FAIL:**
```
Title: "üêô The Octopus üêô Paradox üß†"
Emojis in title: 3 ‚ùå
```

---

## Auto-Rejection Logic

### Rejection Thresholds

**Hard Failures (auto-reject):**
- Fact Accuracy < 5/10 ‚Üí Too risky, potential misinformation
- Total Score < 35/50 ‚Üí Below quality threshold

**Soft Warnings (pass but log):**
- Brand Voice < 7/10 ‚Üí Flag for manual review tone
- Engagement < 6/10 ‚Üí May underperform

### Exit Codes

```python
exit(0)  # PASS - Send to Telegram
exit(1)  # FAIL - Reject and notify
```

### n8n Error Handling

```javascript
// In IF node
const report = JSON.parse($node["Quality Validator"].json.stdout);

if (!report.passed) {
  // Log rejection reason
  console.error(`Rejected: ${report.total_score}/50`);

  // Route to error notification
  return {
    json: {
      error: true,
      score: report.total_score,
      threshold: report.threshold,
      reasons: report.details
    }
  };
}

// Continue to Telegram
return {
  json: {
    passed: true,
    score: report.total_score
  }
};
```

---

## Testing & Examples

### Test Carousel 1: High Quality (Should PASS)

**Input:** `/tmp/test_carousel_good.json`

```json
{
  "id": "test_001",
  "fact": {
    "fact": "Octopuses have three hearts and blue blood due to copper-based hemocyanin",
    "category": "Marine Biology",
    "source_url": "https://www.nature.com/articles/octopus-biology",
    "verified": true
  },
  "carousel": [
    {
      "slide_number": 1,
      "type": "hook",
      "title": "Why Blue Blood?",
      "subtitle": "Octopuses evolved a radically different circulatory system than mammals"
    },
    {
      "slide_number": 2,
      "type": "reveal",
      "title": "Three Hearts",
      "subtitle": "Two pump blood to gills ü´Å, one circulates it through the body"
    },
    {
      "slide_number": 3,
      "type": "reveal",
      "title": "Copper, Not Iron",
      "subtitle": "Hemocyanin uses copper instead of iron, turning blood blue when oxygenated ü©∏"
    },
    {
      "slide_number": 4,
      "type": "reveal",
      "title": "Cold Water Advantage",
      "subtitle": "Copper-based blood carries oxygen more efficiently in cold ocean environments"
    },
    {
      "slide_number": 5,
      "type": "cta",
      "title": "Follow for More",
      "subtitle": "Deep dives into ocean science üåä"
    }
  ]
}
```

**Expected Results:**

```json
{
  "passed": true,
  "total_score": 42.5,
  "scores": {
    "fact_accuracy": 10,     // Well-documented science
    "brand_voice": 9,        // Professional, engaging
    "engagement": 8,         // Strong hook, curiosity gap
    "length_compliance": 10, // All within limits
    "emoji_usage": 10        // Appropriate density, no title emojis
  }
}
```

### Test Carousel 2: Low Quality (Should FAIL)

**Input:** `/tmp/test_carousel_bad.json`

```json
{
  "id": "test_002",
  "fact": {
    "fact": "Humans only use 10% of their brain capacity",
    "category": "Neuroscience",
    "source_url": null,
    "verified": false
  },
  "carousel": [
    {
      "slide_number": 1,
      "type": "hook",
      "title": "üß† SHOCKING Brain Truth!!",
      "subtitle": "You won't BELIEVE what scientists discovered about the human brain!"
    },
    {
      "slide_number": 2,
      "type": "reveal",
      "title": "Only 10% Used?!",
      "subtitle": "This myth has been around for decades but is it actually true or just a lie that everyone believes without question? üò±üò±üò±"
    }
  ]
}
```

**Expected Results:**

```json
{
  "passed": false,
  "total_score": 18.3,
  "scores": {
    "fact_accuracy": 2,      // Debunked myth
    "brand_voice": 3,        // Clickbait tone, off-brand
    "engagement": 6,         // Curiosity but misleading
    "length_compliance": 5,  // Subtitle too long (140+ chars)
    "emoji_usage": 2         // Emojis in title, excessive density
  },
  "details": {
    "fact_accuracy": {
      "concerns": ["Well-known debunked myth", "No credible source"]
    },
    "brand_voice": {
      "improvements": ["Remove hype language", "Use professional tone"]
    }
  }
}
```

### Running Tests

```bash
# Test good carousel
~/ssh-nexus 'python3 /data/scripts/quality_validator.py /tmp/test_carousel_good.json'

# Test bad carousel (should fail)
~/ssh-nexus 'python3 /data/scripts/quality_validator.py /tmp/test_carousel_bad.json'
echo $?  # Should output: 1 (failure)

# Test with custom threshold
~/ssh-nexus 'python3 /data/scripts/quality_validator.py /tmp/test.json --threshold 40'

# Save detailed report
~/ssh-nexus 'python3 /data/scripts/quality_validator.py /tmp/test.json --output /tmp/report.json'
```

---

## Deployment

### Step 1: Deploy Script to Pi

**Copy script via Samba:**

1. Open `\\100.122.207.23\nexus-projects` in Windows Explorer
2. Navigate to `faceless_prod/scripts/`
3. Create `quality_validator.py` with the script content above

**Or via SSH:**

```bash
# From local repo
cat scripts/quality_validator.py | ~/ssh-nexus 'cat > /srv/projects/faceless_prod/scripts/quality_validator.py'

# Make executable
~/ssh-nexus 'chmod +x /srv/projects/faceless_prod/scripts/quality_validator.py'
```

### Step 2: Update Docker Configuration

**Edit `/srv/docker/n8n.Dockerfile`:**

```dockerfile
# Add after existing pip3 install line
RUN pip3 install --no-cache-dir anthropic==0.39.0
```

**Edit `/srv/docker/docker-compose.yml`:**

```yaml
services:
  n8n:
    environment:
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
```

**Edit `/srv/docker/.env`:**

```bash
ANTHROPIC_API_KEY=sk-ant-api03-your-actual-key-here
```

**Rebuild and restart:**

```bash
~/ssh-nexus 'cd /srv/docker && sudo docker compose build n8n && sudo docker compose up -d n8n'
```

### Step 3: Update n8n Workflow

1. Open n8n: http://100.122.207.23:5678
2. Open FactsMind workflow
3. Add "Execute Command" node after composite script
4. Configure validation command (see n8n integration section)
5. Add IF node for pass/fail routing
6. Add error notification node
7. Save and test workflow

### Step 4: Test End-to-End

```bash
# Trigger workflow manually in n8n
# Monitor execution in n8n UI
# Check validation output in /tmp/validation_report.json

# View validation logs
~/ssh-nexus 'sudo docker logs nexus-n8n --tail 100 | grep -A 10 "quality validation"'
```

### Step 5: Monitor & Tune

**Track metrics:**
- Rejection rate (aim for <30%)
- False positives (good content rejected)
- False negatives (bad content approved)

**Adjust thresholds in n8n:**

```javascript
// Lower threshold if rejecting too much good content
--threshold 30

// Raise threshold for stricter quality
--threshold 40
```

**Review rejected content:**

```bash
# Check recent rejections
~/ssh-nexus 'grep -r "FAILED" /tmp/validation_report*.json'
```

---

## API Cost Estimation

**Claude API Pricing (as of 2024):**
- Model: claude-3-5-sonnet-20241022
- Input: $3.00 / 1M tokens
- Output: $15.00 / 1M tokens

**Per Validation:**
- Input: ~1,500 tokens (carousel + prompts)
- Output: ~500 tokens (scores + reasoning)
- **Cost: ~$0.01 per validation**

**Monthly estimates:**
- 90 carousels/month (3/day): **~$0.90/month**
- Very affordable for quality assurance

---

## Troubleshooting

### Error: "anthropic package not installed"

```bash
~/ssh-nexus 'sudo docker exec -it nexus-n8n pip3 install anthropic'
```

### Error: "ANTHROPIC_API_KEY not found"

Check environment variable is set:

```bash
~/ssh-nexus 'sudo docker exec -it nexus-n8n env | grep ANTHROPIC'
```

### Error: "File not found: carousel_data.json"

Ensure carousel data is written before validation:

```javascript
// In n8n, before Quality Validator node
const fs = require('fs');
fs.writeFileSync('/tmp/carousel_data.json', JSON.stringify($json));
```

### All Carousels Failing

Lower threshold temporarily:

```bash
--threshold 25  # Investigate why scores are low
```

### Claude API Rate Limits

Add retry logic in script (already handled by Anthropic SDK)

---

## Future Enhancements

1. **Image Quality Validation**
   - Analyze rendered slides for text readability
   - Check color contrast ratios
   - Verify logo placement

2. **A/B Testing Integration**
   - Track engagement metrics for validated posts
   - Correlate quality scores with actual performance
   - Refine scoring weights based on data

3. **Custom Training**
   - Fine-tune scoring based on historical performance
   - Learn FactsMind-specific patterns
   - Adaptive thresholds

4. **Multi-Model Validation**
   - Use multiple AI models for consensus scoring
   - Reduce single-model bias
   - Higher confidence in scores

5. **Automated Fixes**
   - Suggest specific edits to pass validation
   - Auto-shorten text that's too long
   - Emoji optimization recommendations

---

## Summary

This implementation adds a robust AI-powered quality gate to your FactsMind carousel pipeline:

‚úÖ **Fact-checking** prevents misinformation
‚úÖ **Brand consistency** maintains professional voice
‚úÖ **Engagement optimization** improves performance
‚úÖ **Format validation** ensures technical compliance
‚úÖ **Cost-effective** at ~$0.01 per carousel

**Total LOC:** ~450 lines (script + integration)
**Setup time:** ~30 minutes
**Maintenance:** Minimal (tune thresholds monthly)

Start with a 35/50 threshold and adjust based on rejection rates. Monitor for 2 weeks, then optimize.

---

*Last updated: 2025-11-18*
*Part of Nexus 2.0 Quality Enhancement Initiative*
